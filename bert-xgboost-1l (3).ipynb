{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7379779,"sourceType":"datasetVersion","datasetId":4288635},{"sourceId":11869773,"sourceType":"datasetVersion","datasetId":7459229}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------- Setup & Data --------------------\nimport kagglehub\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# Download dataset\npath = kagglehub.dataset_download(\"shanegerami/ai-vs-human-text\")\nprint(\"📥 Downloaded dataset to:\", path)\ndf = pd.read_csv(f\"{path}/AI_Human.csv\").head(100000)\n\n# Preprocess labels\ndf = df.rename(columns={\"generated\": \"label\"})\ndf[\"label\"] = df[\"label\"].astype(int)\nprint(\"✅ Loaded dataset. Shape:\", df.shape)\n\n# Train/Test split\ntrain_df, test_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)   #is to ensure that the distribution of labels (classes) remains balanced in both the training and testing datasets.\n\n# Save supporting files\noutput_dir = \"/kaggle/working/ai_vs_human_split\"\nos.makedirs(output_dir, exist_ok=True)\ntrain_df.to_csv(f\"{output_dir}/train_essays.csv\", index=False)\ntest_df.to_csv(f\"{output_dir}/test_essays.csv\", index=False)\n\nprint(\"📂 Files saved in:\", output_dir)\n\n# -------------------- TPU Setup --------------------\nimport tensorflow as tf\n\ntry:\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')  # For TPU VM\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n    print(\"✅ TPU initialized.\")\nexcept Exception as e:\n    print(\"🚫 TPU not found or failed to initialize. Using default strategy.\\nError:\", str(e))\n    strategy = tf.distribute.get_strategy()\n\nprint(\"🔧 Strategy in use:\", strategy)\n\n# -------------------- BERT + Tokenization --------------------\nfrom transformers import BertTokenizer, TFBertModel\nfrom tqdm import tqdm\nimport numpy as np\n\nlocal_model_path='/kaggle/input/bert-base-uncased/bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(local_model_path)\n\n# Load BERT model inside strategy scope\nwith strategy.scope():\n    bert_model = TFBertModel.from_pretrained(local_model_path)\n\n# Function to get embeddings using batch + tf.data\ndef get_bert_embeddings_batched(texts, batch_size=32, max_len=128):   #This sets the maximum number of tokens the tokenizer will return for each input. If the input text is longer than 128 tokens, it gets truncated. If it’s shorter, it’s padded (if padding=True is set).\n    print(f\"🔄 Getting BERT embeddings for {len(texts)} texts...\")     #BERT can handle up to 512 tokens\n    encodings = tokenizer(\n        texts,\n        truncation=True,\n        padding='max_length',\n        max_length=max_len,\n        return_tensors='tf'\n    )\n\n    # Save token IDs and masks\n    input_ids = encodings['input_ids'].numpy()\n    attention_mask = encodings['attention_mask'].numpy()\n\n    print(\"📥 Sample token IDs:\\n\", input_ids[:2])\n    print(\"🧠 Sample attention masks:\\n\", attention_mask[:2])\n\n    # Save token ids and attention masks\n    np.save(f\"{output_dir}/input_ids.npy\", input_ids)\n    np.save(f\"{output_dir}/attention_masks.npy\", attention_mask)\n\n    dataset = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\": encodings['input_ids'],\n        \"attention_mask\": encodings['attention_mask']\n    }).batch(batch_size)\n\n    all_embeddings = []\n    for batch in tqdm(dataset, desc=\"⚙️  Running BERT\"):\n        outputs = bert_model(batch)['last_hidden_state'][:, 0, :]  # CLS token\n        all_embeddings.append(outputs)\n\n    full_embeddings = tf.concat(all_embeddings, axis=0).numpy()\n    print(\"📐 Embeddings shape:\", full_embeddings.shape)\n    print(\"🔢 Sample of first embedding (first 10 dims):\", full_embeddings[0][:10])\n\n    return full_embeddings\n\n# Load train and test\ntrain = pd.read_csv(f\"{output_dir}/train_essays.csv\")\ntest = pd.read_csv(f\"{output_dir}/test_essays.csv\")\n\n# Generate embeddings\nX_train_bert = get_bert_embeddings_batched(train['text'].tolist())\nX_test_bert = get_bert_embeddings_batched(test['text'].tolist())\ny_train = train['label'].values\ny_test = test['label'].values\n\n# Save embeddings for reuse\nnp.save(f\"{output_dir}/X_train_bert.npy\", X_train_bert)\nnp.save(f\"{output_dir}/X_test_bert.npy\", X_test_bert)\n\n# -------------------- XGBoost Classifier --------------------\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(\"🚀 Training XGBoost classifier...\")\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nxgb.fit(X_train_bert, y_train)\n\n# -------------------- Evaluation --------------------\n# Evaluate on training data\ntrain_preds = xgb.predict(X_train_bert)\ntrain_acc = accuracy_score(y_train, train_preds)\nprint(\"📊 Training Accuracy:\", train_acc)\nprint(\"🔍 Training Classification Report:\\n\", classification_report(y_train, train_preds))\n\n# Evaluate on test data\ntest_preds = xgb.predict(X_test_bert)\ntest_acc = accuracy_score(y_test, test_preds)\nprint(\"📊 Test Accuracy:\", test_acc)\nprint(\"🔍 Test Classification Report:\\n\", classification_report(y_test, test_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:29:19.302729Z","iopub.execute_input":"2025-05-19T13:29:19.302914Z","iopub.status.idle":"2025-05-19T13:50:22.788677Z","shell.execute_reply.started":"2025-05-19T13:29:19.302896Z","shell.execute_reply":"2025-05-19T13:50:22.787998Z"}},"outputs":[{"name":"stdout","text":"📥 Downloaded dataset to: /kaggle/input/ai-vs-human-text\n✅ Loaded dataset. Shape: (100000, 2)\n📂 Files saved in: /kaggle/working/ai_vs_human_split\n","output_type":"stream"},{"name":"stderr","text":"2025-05-19 13:30:03.211407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747661403.388639      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747661403.440049      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🚫 TPU not found or failed to initialize. Using default strategy.\nError: Please provide a TPU Name to connect to.\n🔧 Strategy in use: <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7edf033c97d0>\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747661425.818580      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"🔄 Getting BERT embeddings for 70000 texts...\n📥 Sample token IDs:\n [[  101  6203  3516  2110  5205  1010  1045  7475  1999  5684  1997  4363\n   1996  6092  2267  1012  1996  6092  2267  2003  3214  2000  2022  1037\n  12014  2090  2602  1997  1996  2343  2011  1037  3789  1999  3519  1998\n   2602  1997  1996  2343  2011  1037  2759  3789  1997  4591  4480  1012\n   1045  7475  2005  4363  1996  6092  2267  2138  1997  2195  4436  6153\n   2039  2007  2592  1012 15847  1010  1045  1005  1040  2066  2000  2391\n   2041  2008  1996  6092  2267  3084  2009  2061  2008  3469  2163  2123\n   1005  1056  2031  3469  3747  2084  3760  2163  1012  2065  1996  6092\n   2267  2003  3718  2059  3469  2163  4618  2031  2172  2062  2576  2373\n   2084  3760  2163  1012  2029  2965  2008  1037  2235  2110  2453  2025\n   2131  1996  2168  3815  1997  3086  2013   102]\n [  101  1999  3522  2086  1010  2116  2816  2031  5625  2000  3749  3454\n   2005  3784  4083  2083  2678  8921  1998  3784 14799  1012  2116  6667\n   3073  2023  2004  2019  5724  2005  2493  8302  2012  2037  2816  1012\n   2348  2009  8440  1005  1056  2042  2004  6923  2426  4732  1010  2690\n   1010  1998  2152  2816  1010  2082  7923  2031  2318  2000  6848  2023\n   5724  2349  2000  1996 21887 23350  8293  1012  2023  5724  4473  5089\n   2000  3613  7899  2096  2025  2383  3622  3967  2007  1996  2493  1998\n  11504 14879  1996  3659  1997  1996  7865  1012  2023  4118  2036 13416\n   1996  3036  3891  1997  2816  2073  2045  2003  1996  6061  1997  1037\n   2543  1010  3019  7071  1010  2030  3161 13108  1012  1999  2070  3572\n   1010  2023  4118  1997  4083  2064  2022   102]]\n🧠 Sample attention masks:\n [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n","output_type":"stream"},{"name":"stderr","text":"⚙️  Running BERT: 100%|██████████| 2188/2188 [06:33<00:00,  5.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"📐 Embeddings shape: (70000, 768)\n🔢 Sample of first embedding (first 10 dims): [-0.0687438  -0.25372472 -0.8853341   0.06881139 -0.0010235  -0.5130544\n  0.3910389   0.9081425  -0.16836181 -0.10408129]\n🔄 Getting BERT embeddings for 30000 texts...\n📥 Sample token IDs:\n [[  101  6203  3836  1035  2171  1010  1996  2095  2003  2249  1010  2471\n   2296  4845  1999  1996  5893  3694  2038  1037  3526  3042  1012  2065\n   2057  2071  2069  2224  2009  2043  2057  4995  1005  1056  1999  2082\n   2084  2054  1005  1055  1996  2391  1029  2065  4268  2020  2583  2000\n   2224  2037  3526 11640  1999  2465  1010  2009  2052  3499  2336  2000\n   3579  2062  1999  2465  1998  2027  2052  4847  1996  3627  1997  3810\n   2037 11640  2125  1999  2465  1012  1045  2903  2008  2009  2052  2022\n   1037  2204  2801  2000  2991  5004  2083  2007  3343  2193  2028  1012\n   2076  2489  6993  1998  6265  2847  1010  2045  2003  2053  7386  1999\n   4352  2493  2000  2224  2037  3526 11640  1012  2138  2045  2003  2053\n   2132 19093  2008  2175  2247  2007  4442   102]\n [  101  4330 11640  2031  2468  2019  9897  2112  1997  2256  3679  3268\n   1010  1998  2009  2003  2053  4474  2008  2027  2024  2036  2556  1999\n  12463  1012  2096  2070  7475  2008  3526 11640  2323  2022  3039  1999\n  12463  1010  2500  2903  2008  2027  2323  2022  7917  1012  1999  2023\n   9491  1010  1045  2097 16157  1996  4013  2015  1998  9530  2015  1997\n   4352  3526 11640  2000  2022  2109  1999 12463  1012  2006  1996  2028\n   2192  1010  2045  2024  2195 12637  2000  4352  3526 11640  1999 12463\n   1012 15847  1010  3526 11640  2064  2022  2109  2004  1037  6994  2005\n   4083  1012  2007  1996 11343  1997  4547 18726  1998  3784  4219  1010\n   2493  2064  2224  2037  3526 11640  2000  3229  2592  1998  3143 14799\n   1012  2023  2064  2022  3391  6179  2005   102]]\n🧠 Sample attention masks:\n [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n","output_type":"stream"},{"name":"stderr","text":"⚙️  Running BERT: 100%|██████████| 938/938 [02:53<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"📐 Embeddings shape: (30000, 768)\n🔢 Sample of first embedding (first 10 dims): [ 0.3196784  -0.26945573  0.16511464  0.28543472  0.31779614 -0.7717479\n  0.5807484   0.6513076  -0.05484775 -0.5954517 ]\n🚀 Training XGBoost classifier...\n📊 Training Accuracy: 0.9998142857142858\n🔍 Training Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     37041\n           1       1.00      1.00      1.00     32959\n\n    accuracy                           1.00     70000\n   macro avg       1.00      1.00      1.00     70000\nweighted avg       1.00      1.00      1.00     70000\n\n📊 Test Accuracy: 0.9791333333333333\n🔍 Test Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98     15875\n           1       0.98      0.97      0.98     14125\n\n    accuracy                           0.98     30000\n   macro avg       0.98      0.98      0.98     30000\nweighted avg       0.98      0.98      0.98     30000\n\n","output_type":"stream"}],"execution_count":1}]}